{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ubUJN-0rEHn",
        "outputId": "c24b5e6e-3ced-4242-8acd-0cdc83d2ff7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "I-2ZygEHrLMT"
      },
      "outputs": [],
      "source": [
        "#1.1\n",
        "# Function to clean and normalize text fields\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # Delete no ASCII character\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces\n",
        "    tokens = text.split() # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmed = [stemmer.stem(word) for word in tokens] # Apply stemming\n",
        "\n",
        "    return ' '.join(stemmed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xtGxTlHJrM0n"
      },
      "outputs": [],
      "source": [
        "#1.2\n",
        "with open('fashion_products_dataset.json', 'r', encoding='utf-8') as f:\n",
        "    corpus = json.load(f)\n",
        "# Text cleaning to title and description fields\n",
        "for doc in corpus:\n",
        "    doc['title_clean'] = clean_text(doc.get('title', ''))\n",
        "    doc['description_clean'] = clean_text(doc.get('description', ''))\n",
        "\n",
        "REQUIRED_FIELDS = [\n",
        "    'pid', 'title', 'description', 'brand', 'category', 'sub_category',\n",
        "    'product_details', 'seller', 'out_of_stock', 'selling_price',\n",
        "    'discount', 'actual_price', 'average_rating', 'url'\n",
        "]\n",
        "# We ensure all required fields are present in each document\n",
        "def ensure_fields(doc):\n",
        "    for field in REQUIRED_FIELDS:\n",
        "        if field not in doc:\n",
        "            doc[field] = None\n",
        "    return doc\n",
        "\n",
        "# Apply field completion to the entire corpus\n",
        "corpus = [ensure_fields(doc) for doc in corpus]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NOAejA9BEb0L"
      },
      "outputs": [],
      "source": [
        "#1.4\n",
        "def normalize_numeric_fields(doc):\n",
        "    # Convert price and rating fields to numeric types\n",
        "    try:\n",
        "        doc['selling_price'] = float(doc['selling_price'].replace(',', '.'))\n",
        "    except:\n",
        "        doc['selling_price'] = None\n",
        "    try:\n",
        "        doc['actual_price'] = float(doc['actual_price'].replace(',', '.'))\n",
        "    except:\n",
        "        doc['actual_price'] = None\n",
        "    try:\n",
        "        doc['discount'] = int(doc['discount'].replace('% off', '').strip())\n",
        "    except:\n",
        "        doc['discount'] = None\n",
        "    try:\n",
        "        doc['average_rating'] = float(doc['average_rating'])\n",
        "    except:\n",
        "        doc['average_rating'] = None\n",
        "    '''\n",
        "    # Ensure out_of_stock is boolean\n",
        "    if isinstance(doc.get('out_of_stock'), str):\n",
        "        doc['out_of_stock'] = doc['out_of_stock'].lower() == 'true'\n",
        "    elif not isinstance(doc.get('out_of_stock'), bool):\n",
        "        doc['out_of_stock'] = None\n",
        "    '''\n",
        "    return doc\n",
        "\n",
        "# Apply normalization to the entire corpus\n",
        "corpus = [normalize_numeric_fields(doc) for doc in corpus]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
